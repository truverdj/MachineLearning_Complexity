---
title: "STA 561 HW5"
author: "Daniel Truver"
date: "3/21/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

#### (1) Hoefding's Inequality 

Let $X$ be a random variable in $[a,b]$ and $t \geq 0$. Then prepare for the string of inequalities.

$$
\begin{aligned}
Pr(X - \mu_X \geq t) 
&\leq \min_{\lambda \geq 0} M_{X - \mu_X}(\lambda)e^{-\lambda t} \quad (\text{Chernoff Bounds}) \\
&= \min_{\lambda\geq 0} E\left[ e^{\lambda(X-\mu_X)} \right]e^{-\lambda t} \quad (\text{MGF Denfinition}) \\
&\leq \min_{\lambda\geq 0} \exp\left( \frac{\lambda^2(b-a)^2}{8} \right) \exp(-\lambda t) \quad \text{(Hoefding Lemma)} \\
&= \min_{\lambda\ge0}\exp\left( \frac{\lambda^2(b-a)^2}{8} - \lambda t \right)
\end{aligned}
$$

We would now like to find the value of lambda that minimizes this function. We employ good old calculus.

$$
\begin{aligned}
0 
&= \frac{d}{d\lambda} \exp\left( \frac{\lambda^2(b-a)^2}{8} - \lambda t \right)\\
&= \exp\left( \frac{\lambda^2(b-a)^2}{8} - \lambda t \right)\left( \frac{2\lambda(b-a)^2}{8} - t \right) \\
\implies \\
0 &= \left( \frac{2\lambda(b-a)^2}{8} - t \right) \\
\lambda^* &= \frac{4t}{(b-a)^2}
\end{aligned}
$$

We shove this value of $\lambda$ back into our expression and obtain:

$$
\begin{aligned}
Pr(X - \mu_X \geq t) 
&\leq \exp\left[ \left( \frac{4t}{(b-a)^2} \right)^2 \frac{(b-a)^2}{8} - \left(\frac{4t}{(b-a)^2}\right)t \right] \\
&= \exp\left( \frac{-2t^2}{(b-a)^2} \right)
\end{aligned}
$$

Then, given a set of independent random variables $X_1,\dots,X_n$, we can say
$$
\prod_{i=1}^n Pr(X_i - \mu_{X_i} \geq t) \leq \prod_{i=1}^n \exp\left( \frac{-2t^2}{(b-a)^2} \right) = \exp\left( -\frac{2nt^2}{(b-a)^2} \right).
$$

We will now do some rearrangement and elementary probability/set theory.

$$
\begin{aligned}
\prod_{i=1}^n Pr(X_i - \mu_{X_i} < t) 
&\geq 1 - \exp\left( -\frac{2nt^2}{(b-a)^2} \right) \\
Pr\left(\sum_{i=1}^n (X_i - \mu_{X_i}) < nt \right) & \geq \prod_{i=1}^n Pr(X_i - \mu_{X_i} < t) 
\end{aligned}
$$

The second inequality is obtained by noting that the righthand side is the probability of the event (A) "every $X_i$ is less than $t$ away from its mean," and the lefthand side is the probability of the event (B) "the sum of $X_i$ deviations from their means is less than $nt$." A is sufficient for B ($A\subset B$) which gives us the inequality in probabilities. So, we now have

$$
\begin{aligned}
Pr\left(\frac{1}{n}\sum_{i=1}^n (X_i - \mu_{X_i}) < t \right) &\geq 1 - \exp\left( -\frac{2nt^2}{(b-a)^2} \right) \\
Pr\left(\frac{1}{n}\sum_{i=1}^n (X_i - \mu_{X_i}) \geq t \right) &\leq \exp\left( -\frac{2nt^2}{(b-a)^2} \right),
\end{aligned}
$$
which we know is Hoefding's Inequality. 

##### (b) Sharper Bound

Let $X_1, \dots, X_n$ be iid standard normal random variables. Then we have 
$$
\frac{1}{n}\sum_{i=1}^n X_i \sim N(0,1/n),
$$
which has a sharper bound. 

#### (2) VC Dimension

##### (a) Find VC Dimensions

$\mathcal{H}_1$ is a space of linear functions in $p$ dimensions, so the VC-dimension is $p+1$.

$\mathcal{H}_2$ is a space of quadratic functions. However, we can expand the kernel $k_2$ is the following way.
$$
(x_i^Tx + 1)^2 = (1 + x_{i1}x_1 + x_{i2}x_2 + \cdots + x_{ip}x_p)^2
$$
Were we to expand this using the multinomial theorem, we would end up with 
$$
1^2 + x_{i1}^2x_1^2 + \cdots + x_{ip}^2x_p^2 + 2x_{i1}x_{i2}x_1x_2 + \dots 
$$
The point is, we'll have
$$
\frac{(2+p)!}{2!p!} = \frac{(p+2)(p+1)}{2} = d
$$
terms, meaning $k_2$ is the inner product between vectors in $d - 1$ dimensions plus an intercept term, here $1$. So, we have a halfspace in $d-1$ dimensions. Therefore, the VC-dimension of $\mathcal{H}_2$ is $d$.

##### (b) Approximation and Estimation Error
